{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stand: 10/2020 | Autor: Fabian\n",
    "\n",
    "# AWS Sagemaker\n",
    "Sagemaker ist ein Service von AWS zum trainieren von Machine Learning Modellen.\n",
    "Hierzu erfüllt Sagemaker unterschiedliche Aufgaben. Eine Teil des Services ist die Bereitstellung von Jupyter Notebook Instanzen, die ganz ähnlich wie Jupyter Notebooks auf einem Laptop verwendet werden können. Wobei Sagemaker bei der Verwendung von weiteren AWS Services wie S3 einige Vorteile bietet, welche im folgenden geziegt werden.\n",
    "\n",
    "Im nächsten Teil wird geziegt, wie die Sagemaker API verwendet werden kann und wie Frameworks wie sklearn und XGB aus  Sagemaker Containern verwendet werden können.\n",
    "\n",
    "\n",
    "# Einstieg in Sagemaker Notebooks\n",
    "\n",
    "Ziel dieses Notebooks ist es die die Vorteile von Sagemaker im Zusammenspiel mit anderen AWS Services aufzuzeigen.\n",
    "\n",
    "Inhalt:\n",
    "\n",
    "1. Jupyter Magic\n",
    "2. Einlesen von Dateien\n",
    "3. Verarbeiten von Daten\n",
    "4. Visualisieren von Daten\n",
    "5. Speicher von Dateien\n",
    "\n",
    "\n",
    "## Importieren von Paketen\n",
    "\n",
    "Genau wie auf deinem Notebook, kannst du bereits installierte Pakete importieren. Hier besteht __kein__ Unterschied zwischen der Nutzung von Jupyter auf deinem Laptop oder in Sagemaker.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import pickle\n",
    "import boto3 # Libary für den Zugriff aus AWS Services wie S3\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sagemaker unterstützt Jupyter Magic\n",
    "Mit den Magic Commands können Behelfe in der Commandline ausgeführt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo hallo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einlesen von Daten\n",
    "Um ein ML Modell trainieren zu können brauchen wir Daten. Welche unterschiedlichen Wege es gibt, Daten zu verwendet wird in diesem Kapitel aufgezeigt.\n",
    "\n",
    "### Beispieldatensätze aus dem Internet laden\n",
    "\n",
    "Daten, welche aus dem Internet geladen werden, können wie auf deinem Laptop auch eingelesen und verarbeitet werden. Hier besteht kein Unterschied zu Sagemaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_boston(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'X: {X.shape}, y: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Pandas:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/Fa-bi-an/Intro_Sagemaker/main/data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 Buckets\n",
    "\n",
    "__1. NICHT serialisierte Dateien direkt aus einem Bucket__<br>\n",
    "Bspw.: `CSV` oder `JSON`-Dateien. <br>\n",
    "Hierzu muss das Sagemaker Notebook, über die erforderliche ExecutionRole, also Berechtigungen für den Bucket verfügen!\n",
    "Diese weist man einem Notebook beim anlegen zu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 Muster\n",
    "\n",
    "bucket ='<bucket-name>'\n",
    "data_key = '<path>'\n",
    "file_name = '<file.type>'\n",
    "data_path = f's3://{bucket}/{data_key}/{file_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{data_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path, sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "source": [
    "__1.2 per Magic Command__"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/'\n",
    "data_path= f's3://{bucket}/{data_key}/{file_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kopiert Dateien aus S3 in den lokalen Ordner der Sagemaker Instanz\n",
    "!aws s3 cp --recursive $s3_path $data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. SERIALISIERTE Dateien__<br>\n",
    "Bspw.: `pickle`-Dateien.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import boto3\n",
    "from io import BytesIO\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 Muster\n",
    "bucket ='<bucket-name>'\n",
    "data_key = '<path>'\n",
    "file_name = '<file.type>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "with BytesIO() as data:\n",
    "    s3.Bucket(bucket).download_fileobj(f\"{data_key}/{file_name}\", data)\n",
    "    data.seek(0)    # move back to the beginning after writing\n",
    "    df = pickle.load(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verarbeiten von Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df = pd.DataFrame(list(zip(preds, y_test)), \n",
    "               columns =['Preds', 'y_test']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisiern von Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"Preds\", y=\"y_test\", data=preds_df )\n",
    "plt.title('Vorhersage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speichern von Dateien\n",
    "\n",
    "__1.Speicher von nicht serialisierte Dateien mit Pandas direkt in S3__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/38154040/save-dataframe-to-csv-directly-to-s3-python\n",
    "\n",
    "from io import StringIO # python3; python2: BytesIO \n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = '<bucket-name>' # already created on S3\n",
    "csv_buffer = StringIO()\n",
    "preds_df.to_csv(csv_buffer)\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3_resource.Object(bucket, '<file.type>').put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. lokal Speichern und danach zu S3 hochladen__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df.to_csv('<file.type>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datei lokal vorhanden\n",
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verwende die Methode upload_data zum hochladen von lokalen Dateien in S3\n",
    "session.upload_data(path='<file.type>',bucket='<bucket-name>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Löschen der lokalen Datei\n",
    "! rm preds.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3. serialisierte Dateien zu S3 hochladen__<br>\n",
    "bspw.: `Pickle`-Dateien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='<filename>'\n",
    "model_object=df\n",
    "\n",
    "\n",
    "def pickle_safe_and_load(model_name,model_object,local_target,s3_target):\n",
    "    pickle.dump(model_object, open( f\"{local_target}/{model_name}.pkl\", \"wb\" ))\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    s3_resource.Object(bucket,f'{s3_target}/{model_name}.pkl').put(Body=open(f\"{local_target}/{model_name}.pkl\", 'rb'))\n",
    "    os.remove(f\"{local_target}/{model_name}.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.7.3 64-bit ('CVenv': conda)",
   "display_name": "Python 3.7.3 64-bit ('CVenv': conda)",
   "metadata": {
    "interpreter": {
     "hash": "4699f4ed17a79abca840eeb40e2946b30fb58bcd3e9c7d4e1914ad8ba98a86a3"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}